{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, we will be attempting to use transfer learning to predict our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from pathlib import Path\n",
    "import imghdr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the data\n",
    "data_dir = \"./BBTrD/000000001111\"\n",
    "image_extensions = [\".png\", \".jpg\", \".jpeg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_labels():\n",
    "    txt_file_path = \"labels.txt\"\n",
    "    labels = []\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            labels.append(line.strip())\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000000001111',\n",
       " '000000011110',\n",
       " '000000101101',\n",
       " '000000111100',\n",
       " '000001001011',\n",
       " '000001011010',\n",
       " '000001101001',\n",
       " '000001111000',\n",
       " '000010000111',\n",
       " '000010010110',\n",
       " '000010100101',\n",
       " '000010110100',\n",
       " '000011000011',\n",
       " '000011010010',\n",
       " '000011100001',\n",
       " '000011110000',\n",
       " '000100001110',\n",
       " '000100101100',\n",
       " '000101001010',\n",
       " '000101101000',\n",
       " '000110000110',\n",
       " '000110100100',\n",
       " '000111000100',\n",
       " '000111100000',\n",
       " '001000001101',\n",
       " '001000011100',\n",
       " '001001001001',\n",
       " '001001011000',\n",
       " '001010000101',\n",
       " '001010010100',\n",
       " '001011000001',\n",
       " '001011010000',\n",
       " '001100001100',\n",
       " '001101001000',\n",
       " '001110000100',\n",
       " '001111000000',\n",
       " '010000001011',\n",
       " '010000011010',\n",
       " '010000101001',\n",
       " '010000111000',\n",
       " '010010000011',\n",
       " '010010010010',\n",
       " '010010100001',\n",
       " '010010110000',\n",
       " '010100001010',\n",
       " '010100101000',\n",
       " '010110000010',\n",
       " '010110100000',\n",
       " '011000001001',\n",
       " '011000011000',\n",
       " '011010000001',\n",
       " '011010010000',\n",
       " '011100001000',\n",
       " '011110000000',\n",
       " '100000000111',\n",
       " '100000010110',\n",
       " '100000100101',\n",
       " '100000110100',\n",
       " '100001000011',\n",
       " '100001010010',\n",
       " '100001100001',\n",
       " '100011100000',\n",
       " '100100000110',\n",
       " '100100100100',\n",
       " '100101000010',\n",
       " '100101100000',\n",
       " '101000000101',\n",
       " '101000010100',\n",
       " '101001000001',\n",
       " '101001010000',\n",
       " '101100000100',\n",
       " '101101000000',\n",
       " '110000000011',\n",
       " '110000010010',\n",
       " '110000100001',\n",
       " '110000110000',\n",
       " '110100000010',\n",
       " '110100100000',\n",
       " '111000000001',\n",
       " '111000010000',\n",
       " '111100000000']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_states = load_labels()\n",
    "game_states = sorted(game_states)\n",
    "game_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 128\n",
    "image_width = 128\n",
    "batch_size = 81\n",
    "\n",
    "def load_data():\n",
    "    # Load Train Data\n",
    "    train_images = tf.keras.utils.image_dataset_from_directory(\"./BBTrD/\", batch_size=batch_size, image_size=(image_height, image_width), shuffle=False)\n",
    "\n",
    "    # Load Test Data\n",
    "    test_images =tf.keras.utils.image_dataset_from_directory(\"./BBTeD/\", batch_size=batch_size, image_size=(image_height, image_width), shuffle=False)\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81 files belonging to 81 classes.\n",
      "Found 34 files belonging to 34 classes.\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000000001111',\n",
       " '000000011110',\n",
       " '000000101101',\n",
       " '000000111100',\n",
       " '000001001011',\n",
       " '000001011010',\n",
       " '000001101001',\n",
       " '000001111000',\n",
       " '000010000111',\n",
       " '000010010110',\n",
       " '000010100101',\n",
       " '000010110100',\n",
       " '000011000011',\n",
       " '000011010010',\n",
       " '000011100001',\n",
       " '000011110000',\n",
       " '000100001110',\n",
       " '000100101100',\n",
       " '000101001010',\n",
       " '000101101000',\n",
       " '000110000110',\n",
       " '000110100100',\n",
       " '000111000100',\n",
       " '000111100000',\n",
       " '001000001101',\n",
       " '001000011100',\n",
       " '001001001001',\n",
       " '001001011000',\n",
       " '001010000101',\n",
       " '001010010100',\n",
       " '001011000001',\n",
       " '001011010000',\n",
       " '001100001100',\n",
       " '001101001000',\n",
       " '001110000100',\n",
       " '001111000000',\n",
       " '010000001011',\n",
       " '010000011010',\n",
       " '010000101001',\n",
       " '010000111000',\n",
       " '010010000011',\n",
       " '010010010010',\n",
       " '010010100001',\n",
       " '010010110000',\n",
       " '010100001010',\n",
       " '010100101000',\n",
       " '010110000010',\n",
       " '010110100000',\n",
       " '011000001001',\n",
       " '011000011000',\n",
       " '011010000001',\n",
       " '011010010000',\n",
       " '011100001000',\n",
       " '011110000000',\n",
       " '100000000111',\n",
       " '100000010110',\n",
       " '100000100101',\n",
       " '100000110100',\n",
       " '100001000011',\n",
       " '100001010010',\n",
       " '100001100001',\n",
       " '100011100000',\n",
       " '100100000110',\n",
       " '100100100100',\n",
       " '100101000010',\n",
       " '100101100000',\n",
       " '101000000101',\n",
       " '101000010100',\n",
       " '101001000001',\n",
       " '101001010000',\n",
       " '101100000100',\n",
       " '101101000000',\n",
       " '110000000011',\n",
       " '110000010010',\n",
       " '110000100001',\n",
       " '110000110000',\n",
       " '110100000010',\n",
       " '110100100000',\n",
       " '111000000001',\n",
       " '111000010000',\n",
       " '111100000000']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 52.7202 - accuracy: 0.0247\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 58.8859 - accuracy: 0.0370\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 64.8935 - accuracy: 0.1235\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 59.9015 - accuracy: 0.1605\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 56.5342 - accuracy: 0.1975\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 50.0474 - accuracy: 0.3086\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 50.6192 - accuracy: 0.3457\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 47.7985 - accuracy: 0.5309\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 49.0251 - accuracy: 0.5185\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 51.2094 - accuracy: 0.5309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c0a5db20>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 81\n",
    "num_epochs = 10\n",
    "# we will be freezing the base model's layers in order to keep the weights in the optimal condition they were trained in\n",
    "for layer in vgg_conv.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# new classifier model, which consists of the old model's layers plus some new ones\n",
    "extra_layers = vgg_conv.output\n",
    "# extra_layers = keras.layers.Conv2D(32, (3, 3), activation='relu')(extra_layers)\n",
    "extra_layers = keras.layers.Flatten()(extra_layers)\n",
    "extra_layers = keras.layers.Dense(1024, activation='relu')(extra_layers)\n",
    "\n",
    "predictions = keras.layers.Dense(num_classes, activation='softmax')(extra_layers)\n",
    "\n",
    "# create the new model\n",
    "new_model = keras.models.Model(inputs=vgg_conv.input, outputs=predictions)\n",
    "\n",
    "# compile the model\n",
    "new_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "new_model.fit(train_images, epochs=num_epochs)\n",
    "\n",
    "# print(train_images)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 59.2527 - accuracy: 0.0370\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 59.8752 - accuracy: 0.0123\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 53.6311 - accuracy: 0.1358\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 51.5611 - accuracy: 0.1358\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 43.9137 - accuracy: 0.2963\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 38.2325 - accuracy: 0.3580\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 34.1511 - accuracy: 0.3580\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 26.0233 - accuracy: 0.5185\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 21.3180 - accuracy: 0.5432\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: 17.2067 - accuracy: 0.6914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2916dd730>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing dropout\n",
    "# new classifier model, which consists of the old model's layers plus some new ones\n",
    "extra_layers = vgg_conv.output\n",
    "# extra_layers = keras.layers.Conv2D(32, (3, 3), activation='relu')(extra_layers)\n",
    "extra_layers = keras.layers.Flatten()(extra_layers)\n",
    "extra_layers = keras.layers.Dense(1024, activation='relu')(extra_layers)\n",
    "extra_layers = keras.layers.Dropout(0.2)(extra_layers)\n",
    "\n",
    "predictions = keras.layers.Dense(num_classes, activation='softmax')(extra_layers)\n",
    "\n",
    "# create the new model\n",
    "dropout_model = keras.models.Model(inputs=vgg_conv.input, outputs=predictions)\n",
    "\n",
    "# compile the model\n",
    "dropout_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "dropout_model.fit(train_images, epochs=num_epochs)\n",
    "\n",
    "# print(train_images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize & Rescale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    keras.layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "train_images = train_images.map(lambda x, y: (resize_and_rescale(x), y))\n",
    "test_images = train_images.map(lambda x, y: (resize_and_rescale(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "data_flip = tf.keras.Sequential([\n",
    "  keras.layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "])\n",
    "\n",
    "data_rotation = tf.keras.Sequential([\n",
    "  keras.layers.RandomRotation(0.2)\n",
    "])\n",
    "\n",
    "data_contrast = tf.keras.Sequential([\n",
    "  keras.layers.RandomContrast(0.2)\n",
    "])\n",
    "\n",
    "data_shift = tf.keras.Sequential([\n",
    "  keras.layers.RandomTranslation(0.2, 0.2)\n",
    "])\n",
    "\n",
    "flipped_images = train_images.map(lambda x, y: (data_flip(x), y))\n",
    "rotated_images = train_images.map(lambda x, y: (data_rotation(x), y))\n",
    "contrast_images = train_images.map(lambda x, y: (data_contrast(x), y))\n",
    "shifted_images = train_images.map(lambda x, y: (data_shift(x), y))\n",
    "\n",
    "train_images = train_images.concatenate(flipped_images)\n",
    "train_images = train_images.concatenate(rotated_images)\n",
    "train_images = train_images.concatenate(contrast_images)\n",
    "train_images = train_images.concatenate(shifted_images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 5.0433 - accuracy: 0.1160\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 4.2238 - accuracy: 0.3062\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 3.4724 - accuracy: 0.4519\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 2.9041 - accuracy: 0.5728\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 2.4486 - accuracy: 0.6568\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 2.0662 - accuracy: 0.7111\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 16s 3s/step - loss: 1.7862 - accuracy: 0.7457\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 16s 3s/step - loss: 1.5879 - accuracy: 0.7333\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.2837 - accuracy: 0.8272\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.1123 - accuracy: 0.8346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c0e24f70>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test normal model\n",
    "new_model.fit(train_images, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 3.8028 - accuracy: 0.2000\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 3.4015 - accuracy: 0.3358\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 2.9399 - accuracy: 0.5062\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 16s 3s/step - loss: 2.6278 - accuracy: 0.5531\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 2.1989 - accuracy: 0.6494\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.8551 - accuracy: 0.6889\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.6584 - accuracy: 0.7210\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.4874 - accuracy: 0.7654\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.3314 - accuracy: 0.7358\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 15s 3s/step - loss: 1.1914 - accuracy: 0.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c094c5b0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dropout model\n",
    "dropout_model.fit(train_images, epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
