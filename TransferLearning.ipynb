{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, we will be attempting to use transfer learning to predict our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from pathlib import Path\n",
    "import imghdr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the data\n",
    "data_dir = \"./BBTrD/000000001111\"\n",
    "image_extensions = [\".png\", \".jpg\", \".jpeg\"]  # add there all your images file extensions\n",
    "\n",
    "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
    "for filepath in Path(data_dir).rglob(\"*\"):\n",
    "    if filepath.suffix.lower() in image_extensions:\n",
    "        img_type = imghdr.what(filepath)\n",
    "        if img_type is None:\n",
    "            print(f\"{filepath} is not an image\")\n",
    "        elif img_type not in img_type_accepted_by_tf:\n",
    "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_labels():\n",
    "    txt_file_path = \"labels.txt\"\n",
    "    labels = []\n",
    "    with open(txt_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            labels.append(line.strip())\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000000001111',\n",
       " '000000011110',\n",
       " '000000101101',\n",
       " '000000111100',\n",
       " '000001001011',\n",
       " '000001011010',\n",
       " '000001101001',\n",
       " '000001111000',\n",
       " '000010000111',\n",
       " '000010010110',\n",
       " '000010100101',\n",
       " '000010110100',\n",
       " '000011000011',\n",
       " '000011010010',\n",
       " '000011100001',\n",
       " '000011110000',\n",
       " '000100001110',\n",
       " '000100101100',\n",
       " '000101001010',\n",
       " '000101101000',\n",
       " '000110000110',\n",
       " '000110100100',\n",
       " '000111000100',\n",
       " '000111100000',\n",
       " '001000001101',\n",
       " '001000011100',\n",
       " '001001001001',\n",
       " '001001011000',\n",
       " '001010000101',\n",
       " '001010010100',\n",
       " '001011000001',\n",
       " '001011010000',\n",
       " '001100001100',\n",
       " '001101001000',\n",
       " '001110000100',\n",
       " '001111000000',\n",
       " '010000001011',\n",
       " '010000011010',\n",
       " '010000101001',\n",
       " '010000111000',\n",
       " '010010000011',\n",
       " '010010010010',\n",
       " '010010100001',\n",
       " '010010110000',\n",
       " '010100001010',\n",
       " '010100101000',\n",
       " '010110000010',\n",
       " '010110100000',\n",
       " '011000001001',\n",
       " '011000011000',\n",
       " '011010000001',\n",
       " '011010010000',\n",
       " '011100001000',\n",
       " '011110000000',\n",
       " '100000000111',\n",
       " '100000010110',\n",
       " '100000100101',\n",
       " '100000110100',\n",
       " '100001000011',\n",
       " '100001010010',\n",
       " '100001100001',\n",
       " '100011100000',\n",
       " '100100000110',\n",
       " '100100100100',\n",
       " '100101000010',\n",
       " '100101100000',\n",
       " '101000000101',\n",
       " '101000010100',\n",
       " '101001000001',\n",
       " '101001010000',\n",
       " '101100000100',\n",
       " '101101000000',\n",
       " '110000000011',\n",
       " '110000010010',\n",
       " '110000100001',\n",
       " '110000110000',\n",
       " '110100000010',\n",
       " '110100100000',\n",
       " '111000000001',\n",
       " '111000010000',\n",
       " '111100000000']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_states = load_labels()\n",
    "game_states = sorted(game_states)\n",
    "game_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 128\n",
    "image_width = 128\n",
    "batch_size = 81\n",
    "\n",
    "def load_data():\n",
    "    # Load Train Data\n",
    "    train_images = tf.keras.utils.image_dataset_from_directory(\"./BBTrD/\", batch_size=batch_size, image_size=(image_height, image_width), shuffle=False)\n",
    "\n",
    "    # Load Test Data\n",
    "    test_images =tf.keras.utils.image_dataset_from_directory(\"./BBTeD/\", batch_size=batch_size, image_size=(image_height, image_width), shuffle=False)\n",
    "    return train_images, test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81 files belonging to 81 classes.\n",
      "Found 34 files belonging to 34 classes.\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000000001111',\n",
       " '000000011110',\n",
       " '000000101101',\n",
       " '000000111100',\n",
       " '000001001011',\n",
       " '000001011010',\n",
       " '000001101001',\n",
       " '000001111000',\n",
       " '000010000111',\n",
       " '000010010110',\n",
       " '000010100101',\n",
       " '000010110100',\n",
       " '000011000011',\n",
       " '000011010010',\n",
       " '000011100001',\n",
       " '000011110000',\n",
       " '000100001110',\n",
       " '000100101100',\n",
       " '000101001010',\n",
       " '000101101000',\n",
       " '000110000110',\n",
       " '000110100100',\n",
       " '000111000100',\n",
       " '000111100000',\n",
       " '001000001101',\n",
       " '001000011100',\n",
       " '001001001001',\n",
       " '001001011000',\n",
       " '001010000101',\n",
       " '001010010100',\n",
       " '001011000001',\n",
       " '001011010000',\n",
       " '001100001100',\n",
       " '001101001000',\n",
       " '001110000100',\n",
       " '001111000000',\n",
       " '010000001011',\n",
       " '010000011010',\n",
       " '010000101001',\n",
       " '010000111000',\n",
       " '010010000011',\n",
       " '010010010010',\n",
       " '010010100001',\n",
       " '010010110000',\n",
       " '010100001010',\n",
       " '010100101000',\n",
       " '010110000010',\n",
       " '010110100000',\n",
       " '011000001001',\n",
       " '011000011000',\n",
       " '011010000001',\n",
       " '011010010000',\n",
       " '011100001000',\n",
       " '011110000000',\n",
       " '100000000111',\n",
       " '100000010110',\n",
       " '100000100101',\n",
       " '100000110100',\n",
       " '100001000011',\n",
       " '100001010010',\n",
       " '100001100001',\n",
       " '100011100000',\n",
       " '100100000110',\n",
       " '100100100100',\n",
       " '100101000010',\n",
       " '100101100000',\n",
       " '101000000101',\n",
       " '101000010100',\n",
       " '101001000001',\n",
       " '101001010000',\n",
       " '101100000100',\n",
       " '101101000000',\n",
       " '110000000011',\n",
       " '110000010010',\n",
       " '110000100001',\n",
       " '110000110000',\n",
       " '110100000010',\n",
       " '110100100000',\n",
       " '111000000001',\n",
       " '111000010000',\n",
       " '111100000000']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 20s 20s/step - loss: 48.9703 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 20s 20s/step - loss: 65.0389 - accuracy: 0.0617\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 19s 19s/step - loss: 61.6813 - accuracy: 0.1605\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 19s 19s/step - loss: 63.3740 - accuracy: 0.2099\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 19s 19s/step - loss: 63.8710 - accuracy: 0.2222\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 19s 19s/step - loss: 58.5757 - accuracy: 0.3951\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 56.8728 - accuracy: 0.3580\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 21s 21s/step - loss: 55.7819 - accuracy: 0.4444\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 53.8153 - accuracy: 0.5062\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 51.9743 - accuracy: 0.6049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1217c3d30>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 81\n",
    "num_epochs = 10\n",
    "# we will be freezing the base model's layers in order to keep the weights in the optimal condition they were trained in\n",
    "for layer in vgg_conv.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# new classifier model, which consists of the old model's layers plus some new ones\n",
    "extra_layers = vgg_conv.output\n",
    "# extra_layers = keras.layers.Conv2D(32, (3, 3), activation='relu')(extra_layers)\n",
    "extra_layers = keras.layers.Flatten()(extra_layers)\n",
    "extra_layers = keras.layers.Dense(1024, activation='relu')(extra_layers)\n",
    "\n",
    "predictions = keras.layers.Dense(num_classes, activation='softmax')(extra_layers)\n",
    "\n",
    "# create the new model\n",
    "new_model = keras.models.Model(inputs=vgg_conv.input, outputs=predictions)\n",
    "\n",
    "# compile the model\n",
    "new_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "new_model.fit(train_images, epochs=num_epochs)\n",
    "\n",
    "# print(train_images)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 21s 21s/step - loss: 48.9387 - accuracy: 0.0123\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 45.7104 - accuracy: 0.0864\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 49.2377 - accuracy: 0.1605\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 48.5817 - accuracy: 0.2222\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 42.3200 - accuracy: 0.3086\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 36.6406 - accuracy: 0.3704\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 20s 20s/step - loss: 29.1919 - accuracy: 0.4198\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 24.6044 - accuracy: 0.5185\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 17.6654 - accuracy: 0.5679\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 12.7549 - accuracy: 0.6790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1219ad9c0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing dropout\n",
    "# new classifier model, which consists of the old model's layers plus some new ones\n",
    "extra_layers = vgg_conv.output\n",
    "# extra_layers = keras.layers.Conv2D(32, (3, 3), activation='relu')(extra_layers)\n",
    "extra_layers = keras.layers.Flatten()(extra_layers)\n",
    "extra_layers = keras.layers.Dense(1024, activation='relu')(extra_layers)\n",
    "extra_layers = keras.layers.Dropout(0.2)(extra_layers)\n",
    "\n",
    "predictions = keras.layers.Dense(num_classes, activation='softmax')(extra_layers)\n",
    "\n",
    "# create the new model\n",
    "dropout_model = keras.models.Model(inputs=vgg_conv.input, outputs=predictions)\n",
    "\n",
    "# compile the model\n",
    "dropout_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "dropout_model.fit(train_images, epochs=num_epochs)\n",
    "\n",
    "# print(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 8.5352 - accuracy: 0.7284\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 5.5952 - accuracy: 0.8395\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 4.2947 - accuracy: 0.8519\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 3.5760 - accuracy: 0.8765\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 4.7760 - accuracy: 0.7901\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 4.8036 - accuracy: 0.8025\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 18s 18s/step - loss: 2.0577 - accuracy: 0.9136\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 1.2691 - accuracy: 0.9136\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.9412 - accuracy: 0.9012\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 17s 17s/step - loss: 0.7351 - accuracy: 0.9259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1218edf60>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_model.fit(train_images, epochs=num_epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize & Rescale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    keras.layers.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "train_images = train_images.map(lambda x, y: (resize_and_rescale(x), y))\n",
    "test_images = train_images.map(lambda x, y: (resize_and_rescale(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "data_flip = tf.keras.Sequential([\n",
    "  keras.layers.RandomFlip(\"horizontal_and_vertical\")\n",
    "])\n",
    "\n",
    "data_rotation = tf.keras.Sequential([\n",
    "  keras.layers.RandomRotation(0.2)\n",
    "])\n",
    "\n",
    "flipped_images = train_images.map(lambda x, y: (data_flip(x), y))\n",
    "rotated_images = train_images.map(lambda x, y: (data_rotation(x), y))\n",
    "\n",
    "train_images = train_images.concatenate(flipped_images)\n",
    "train_images = train_images.concatenate(rotated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 55s 13s/step - loss: 5.3542 - accuracy: 0.0658\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 51s 13s/step - loss: 4.9281 - accuracy: 0.0988\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 52s 14s/step - loss: 4.5048 - accuracy: 0.1687\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 50s 13s/step - loss: 4.1060 - accuracy: 0.2428\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 51s 13s/step - loss: 3.7688 - accuracy: 0.2963\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 51s 14s/step - loss: 3.4213 - accuracy: 0.3909\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 52s 13s/step - loss: 3.1864 - accuracy: 0.4156\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 58s 15s/step - loss: 2.9437 - accuracy: 0.4774\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 53s 14s/step - loss: 2.7402 - accuracy: 0.5432\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 55s 14s/step - loss: 2.4581 - accuracy: 0.6173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1217580a0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test normal model\n",
    "new_model.fit(train_images, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 52s 14s/step - loss: 4.1986 - accuracy: 0.0823\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 58s 14s/step - loss: 3.9309 - accuracy: 0.1523\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 49s 13s/step - loss: 3.8945 - accuracy: 0.1934\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 46s 13s/step - loss: 3.7293 - accuracy: 0.2016\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 49s 14s/step - loss: 3.5191 - accuracy: 0.3292\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 53s 15s/step - loss: 3.2785 - accuracy: 0.3374\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 53s 14s/step - loss: 3.1235 - accuracy: 0.3621\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 53s 14s/step - loss: 2.8366 - accuracy: 0.4486\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 55s 14s/step - loss: 2.8375 - accuracy: 0.4074\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 54s 14s/step - loss: 2.6036 - accuracy: 0.4527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12040e560>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dropout model\n",
    "dropout_model.fit(train_images, epochs=num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
